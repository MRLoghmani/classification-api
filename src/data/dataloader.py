import os
from typing import List, Dict
import tensorflow as tf
import tensorflow_datasets as tfds
from . import augmentations
import yaml


# Some of the common image classification datasets from tfds are added here. Exhaustive list can be found here
# (https://www.tensorflow.org/datasets/catalog/overview)

TENSORFLOW_DATASET_NAMES = [
    "caltech101",
    "caltech_birds2010",
    "caltech_birds2011",
    "cars196",
    "cats_vs_dogs",
    "cifar10",
    "cifar10_1",  # Test set for Cifar  10
    "cifar10_corrupted",  # Generated by adding 15 common corruptions 4 extra corruptions to the test images in Cifar10
    "cifar100",
    "fashion_mnist",
    "food101",
    "imagenette/full-size-v2",    # Imagenette is a subset of imagnet dataset with 10 classes (9,469 train images and 3,925 validation images)
    "i_naturalist2017",
    "mnist",
    "mnist_corrupted",  # Generated by adding 15 common corruptions 4 extra corruptions to the test images in mnist
    "omniglot",
    "oxford_flowers102",
    "oxford_iiit_pet"
]

arguments = [
    ['list[str]', 'train_list', ['RandomCropThenResize', 'RandomHorizontalFlip', 'Normalize'], 'List all the data augmentations separated by comma for training data'],
    ['list[str]', 'val_list', ['Resize', 'CentralCrop', 'Normalize'], 'List all the data augmentations separated by comma for validation data'],
    [str, "data_dir", '', "directory to read/write data. Defaults to  \"~/tensorflow_datasets\""],
    [str, 'name', 'imagenet', 'Choose the dataset to be used for training'],
    [str, 'train_split_id', 'train', ''],
    [str, 'val_split_id', 'validation', ''],
    [int, 'batch_size', 0, 'The size of batch per gpu', lambda x: x > 0],
] + augmentations.arguments


def get_map_fn(transformation_list: List[str], param_dict: Dict, n_classes: int):
  """return the map function applying data augmentation to image and transforming label to one hot vector

  Args:
    param_dict: dict containing transformation name as key and list of corresponding parameters as value
  """
  def map_fn(image, label):
    label = tf.one_hot(label, n_classes)
    image = augmentations.apply_list_of_transformations(image, transformation_list, param_dict)
    return image, label
  return map_fn


class TFRecordExtractor:
  """
  This class extracts the stored tfrecord from the specified directory based on their split (train/val/test)
  and creates a tensorflow dataset object. It assumes that all the tfrecord files are stored in the same directory than the dataset
  meta-data file named `dataset_info.yaml`
  """

  def __init__(self, dataset_name, data_dir, split):
    if not os.path.exists(data_dir):
      raise ValueError(f"there is no directory by {data_dir}")
    if not os.path.exists(os.path.join(data_dir, dataset_name)):
      raise ValueError(f"there is no dataset by the name of {dataset_name} in directory {data_dir}")
    self.data_dir = os.path.join(data_dir, dataset_name)
    self.split = split

  def __get_tfrecord_files_from_dataset_info_file(self):
    """
    get the name of all tfrecord files in the dataset meta-data file named `dataset_info.yaml`
    """
    yaml_file = os.path.join(self.data_dir, 'dataset_info.yaml')
    with open(yaml_file, 'r') as stream:
      try:
        dataset_info = yaml.safe_load(stream)
      except yaml.YAMLError as e:
        print('Error parsing file', yaml_file)
        raise e
    tfrecord_files = [os.path.join(self.data_dir, path) for path in
                      dataset_info["splits"][self.split]["tfrecord_files"]]
    return tfrecord_files

  def __extract_fn(self, tfrecord):
    """Extract tfrecord and decode it to image and label
    """
    feature_description = {
        'image': tf.io.FixedLenFeature([], tf.string),
        'label': tf.io.FixedLenFeature([], tf.int64),
        'size': tf.io.FixedLenFeature([2], tf.int64)
    }
    # Extract the data record
    sample = tf.io.parse_single_example(tfrecord, feature_description)
    image = tf.io.decode_image(sample['image'], channels=3)
    image = tf.reshape(image, [sample['size'][0], sample['size'][1], 3])  # TODO this line should be useless ?
    label = sample['label']
    return (image, label)

  def get_tf_dataset(self):
    """Creates tensorflow dataset object from `tfrecord_files`
    """
    tfrecord_files = self.__get_tfrecord_files_from_dataset_info_file()
    return tf.data.TFRecordDataset(tfrecord_files).map(self.__extract_fn)


def is_training(config, split):
  return split == config['train_split_id']


def get_dataset_from_custom_tfrecord(config, transformation_list: List[str], num_classes: int, split,
                                     num_parallel_calls=tf.data.experimental.AUTOTUNE, buffer_multiplier=15):
  """load a custom dataset from tf record
  """
  map_fn = get_map_fn(transformation_list, config, num_classes)
  # list_files shuffle the files name
  dataset = TFRecordExtractor(config['name'], config['data_dir'], split).get_tf_dataset()

  if is_training(config, split):
    dataset = dataset.shuffle(config['batch_size'] * buffer_multiplier)
  dataset = dataset.map(map_fn, num_parallel_calls=num_parallel_calls).\
      batch(config['batch_size']).\
      prefetch(tf.data.experimental.AUTOTUNE)
  return dataset


def get_dataset_from_tfds(config, transformation_list: List[str], num_classes: int, split,
                          num_parallel_calls=tf.data.experimental.AUTOTUNE, buffer_multiplier=15):
  """load a dataset managed by tfds

  Args:
    config : Dict containing 'name', 'data_dir', 'batch_size'
    split: 'train' or 'test'
  """
  dataset = tfds.load(name=config['name'], split=split, data_dir=config['data_dir'],
                      shuffle_files=is_training, as_supervised=True)
  map_fn = get_map_fn(transformation_list, config, num_classes)
  if is_training(config, split):
    dataset = dataset.shuffle(config['batch_size'] * buffer_multiplier)

  dataset = dataset.map(map_fn, num_parallel_calls=num_parallel_calls).\
      batch(config['batch_size']).\
      prefetch(tf.data.experimental.AUTOTUNE)
  return dataset


# TODO test with autotune, else replace to a value in conf
def get_dataset(config, transformation_list: List[str], num_classes: int, split,
                          num_parallel_calls=tf.data.experimental.AUTOTUNE, buffer_multiplier=15):
  get_function = get_dataset_from_tfds if config["name"] in TENSORFLOW_DATASET_NAMES else get_dataset_from_custom_tfrecord
  return get_function(config, transformation_list, num_classes, split, num_parallel_calls, buffer_multiplier)
